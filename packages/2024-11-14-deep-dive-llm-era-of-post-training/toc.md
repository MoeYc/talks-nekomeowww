# 深入浅出 LLM 3：训练后范式的开启

## 内容大纲与演示流程

1. **前作介绍**
   - *视觉展示*：展示了前两场讲座的封面图与二维码
   - *连续性说明*：将本次讲座定位为系列的第三部分
   - *设计风格*：保持了一致的视觉语言和色彩方案

2. **机器学习基础回顾**
   - *视觉层递*：从向量矩阵的动画展示开始
   - *概念引入*：使用Hinton图展示模型的基本构成
   - *简化表达*：将复杂的机器学习模型简化为矩阵和向量的集合
   - *硬件关联*：展示数据如何送入GPU/CPU进行计算

3. **LLM性能瓶颈分析**
   - *结构化问题陈述*：分为三个主要挑战
   - *视觉设计*：使用统一的卡片样式，有颜色区分的边框
   - *核心问题*：
     - 参数过多（Llama 3.1最大模型有4050亿参数）
     - 线性计算（自回归特性导致的重复计算）
     - 无法动态更新（训练后模型无法获取新知识）

4. **改进方向总览**
   - *三分法*：将改进方向分为三个主要方向
   - *视觉对比*：使用上下箭头直观表示增减变化
   - *焦点领域*：参数压缩、减少重复计算、并行推理

5. **参数压缩与量化**
   - *技术列举*：提出六种参数压缩方法，从剪枝到紧凑网络
   - *视觉隐喻*：使用多边形图形随点击逐渐简化，表示压缩过程
   - *数据支持*：展示量化对模型性能的影响图表
   - *性能数据*：展示FP8量化的具体性能提升
     - FTT减少8.5%
     - 输出速度提高33%
     - 吞吐量增加31%
     - TPM成本降低24%

6. **KV Cache技术解析**
   - *基础回顾*：通过三张图解释注意力机制和掩码原理
   - *核心思想*：空间换时间的策略
   - *优势阐述*：
     - 减少GPU重复计算
     - 一次计算多次复用
     - 无感集成且不影响精度
   - *性能数据*：与现有方案的对比
     - 相比transformers提高24倍
     - 相比TGI提高4倍
     - 内存消耗仅增加43%

7. **预测输出/投机采样**
   - *视频演示*：展示投机采样的工作原理
   - *技术原理*：并行预测多个tokens，加速生成过程
   - *性能数据*：
     - GPT-4o推理速度提高37%
     - OpenAI推理费用增加72%
     - Phi级别小模型速度提高4倍
   - *行业应用*：OpenAI、HuggingFace、Anthropic和vLLM的实现对比

8. **语义化缓存**
   - *概念类比*：将传统CDN缓存概念应用于LLM
   - *技术区别*：
     - 从字节流缓存到Token数据缓存
     - 从基于Hash的缓存到基于语义向量的缓存
   - *优势分析*：
     - 命中缓存可完全消除推理时间
     - 支持无状态分布式部署
     - 网关层集成，兼容任意模型
   - *性能提升*：
     - 重复对话场景提速99%
     - 节约推理开销95%
   - *实用性强调*：无需复杂部署，Redis即可实现

9. **命令式提示词/Codebook**
   - *概念介绍*：LLM负责决策，现实世界执行
   - *应用案例*：机器人控制JSON命令示例
   - *技术定位*：Codebook作为技能列表和能力实现
   - *与Function Calling的区别*：
     - 更适合资源受限环境
     - 只需简单文本能力
     - 可通过透明外部代码控制最终结果

10. **总结与展望**
    - *五大技术总结*：压缩量化、KV Cache、预测输出、语义化缓存、Codebook
    - *视觉统一*：使用一致的卡片设计和图标
    - *整体脉络*：从模型减重到运行加速再到能力拓展
    - *技术融合*：展示各技术如何互补协作

11. **结语**
    - *双语致谢*：中英文谢谢
    - *资源共享*：提供演示文稿源码链接
    - *工具介绍*：提及使用sli.dev构建
    - *二维码*：提供演示文稿获取方式

**演示风格特点：**

- 使用渐进式动画揭示内容，控制信息流
- 大量使用视觉隐喻和图形化展示技术原理
- 每个技术点配以具体性能数据，增强说服力
- 通过颜色编码区分不同技术领域
- 引用多个权威来源支持论点
- 从基础概念逐步深入到前沿技术
- 保持简洁清晰的视觉语言，减少文字密度
- 使用中英双语标题，增强专业性和可读性
